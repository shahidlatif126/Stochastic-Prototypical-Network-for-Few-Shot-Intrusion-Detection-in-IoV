{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Imports and Setup**"
      ],
      "metadata": {
        "id": "7a7z3gnhLTYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "import random\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Libraries loaded. Setup complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZJuKUkdLdxi",
        "outputId": "ae5e9aef-d2bf-4deb-ab71-83e1a1774ce3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries loaded. Setup complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Security & Aggregation Classes:** This cell implements the core components defined in your paper: Schnorr ZKP, Laplace DP, and Median Aggregation."
      ],
      "metadata": {
        "id": "zOS_BHUgLgBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Zero-Knowledge Proof (Schnorr Protocol) ---\n",
        "# Client Authentication\n",
        "class SchnorrProtocol:\n",
        "    def __init__(self, p, g):\n",
        "        self.p = p\n",
        "        self.g = g\n",
        "        self.private_key = np.random.randint(1, p)\n",
        "        self.public_key = pow(g, self.private_key, p)\n",
        "\n",
        "    def generate_commitment(self):\n",
        "        self.r = np.random.randint(1, self.p)\n",
        "        self.R = pow(self.g, self.r, self.p)\n",
        "        return self.R\n",
        "\n",
        "    def compute_response(self, challenge):\n",
        "        self.s = (self.r + challenge * self.private_key) % (self.p - 1)\n",
        "        return self.s\n",
        "\n",
        "    def verify(self, R, s, public_key, challenge):\n",
        "        lhs = pow(self.g, s, self.p)\n",
        "        rhs = (R * pow(public_key, challenge, self.p)) % self.p\n",
        "        return lhs == rhs\n",
        "\n",
        "# --- 2. Differential Privacy (Laplace Mechanism) ---\n",
        "# Implements Section II-B: Privacy Preservation on Predictions\n",
        "def apply_laplace_noise(values, epsilon, sensitivity=1.0):\n",
        "    if epsilon <= 0: return values\n",
        "    scale = sensitivity / epsilon\n",
        "    noise = np.random.laplace(0, scale, size=values.shape)\n",
        "    return values + noise\n",
        "\n",
        "# --- 3. Robust Aggregation (Median) ---\n",
        "# Implements Section II-F: Mitigation of Poisoning Attacks\n",
        "def robust_aggregation(updates):\n",
        "    return np.median(updates, axis=0)"
      ],
      "metadata": {
        "id": "RElT50GwLnsU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Data Loading & Preprocessing**"
      ],
      "metadata": {
        "id": "rGjTNfkyLp8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update this path to your specific dataset location\n",
        "dataset_path = '/content/drive/MyDrive/IDS-IOT2024/Process_1 IDS-IoT-2024.csv'\n",
        "\n",
        "# Check if file exists to prevent errors\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(f\"Error: Dataset not found at {dataset_path}. Please upload the file.\")\n",
        "else:\n",
        "    df = pd.read_csv(dataset_path)\n",
        "\n",
        "    # Feature and Target Extraction\n",
        "    X = df.iloc[:, :-1].values\n",
        "    y = df['Attack_Category_x'].values\n",
        "\n",
        "    # Encode target variable\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "    # Split data (Keeping test set separate for global evaluation)\n",
        "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "        X, y_encoded, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Shuffle for IID simulation\n",
        "    shuffled_indices = np.random.permutation(len(X_train_full))\n",
        "    X_train_shuffled = X_train_full[shuffled_indices]\n",
        "    y_train_shuffled = y_train_full[shuffled_indices]\n",
        "\n",
        "    print(\"Data loaded and preprocessed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBFAnyrELtNw",
        "outputId": "1f33fee2-8a65-42c3-d791-217bf4959660"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded and preprocessed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Main Experiment Loop**"
      ],
      "metadata": {
        "id": "hZ82J4pqLvj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Simulation Parameters ---\n",
        "client_counts = [10, 25, 50, 75, 100]\n",
        "#client_counts = [10]\n",
        "rounds = 10\n",
        "privacy_budget = 1.5  # Epsilon\n",
        "\n",
        "# Schnorr Parameters\n",
        "p = 104729\n",
        "g = 2\n",
        "\n",
        "# LightGBM Parameters\n",
        "lgb_params = {\n",
        "    'objective': 'multiclass',\n",
        "    'num_class': len(np.unique(y_encoded)),\n",
        "    'boosting_type': 'gbdt',\n",
        "    'metric': 'multi_logloss',\n",
        "    'learning_rate': 0.1,\n",
        "    'num_leaves': 31,\n",
        "    'max_depth': -1,\n",
        "    'verbosity': -1\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for client_count in client_counts:\n",
        "    print(f\"\\nRunning experiment for {client_count} clients...\")\n",
        "\n",
        "    # 1. Distribute Data\n",
        "    client_data = []\n",
        "    samples_per_client = len(X_train_shuffled) // client_count\n",
        "    for i in range(client_count):\n",
        "        start_idx = i * samples_per_client\n",
        "        end_idx = start_idx + samples_per_client if i < client_count - 1 else len(X_train_shuffled)\n",
        "        X_client = X_train_shuffled[start_idx:end_idx]\n",
        "        y_client = y_train_shuffled[start_idx:end_idx]\n",
        "        client_data.append((X_client, y_client))\n",
        "\n",
        "    # 2. Assign Adversarial Roles\n",
        "    # Type A: Unverified Clients (Fail ZKP)\n",
        "    num_unverified = min(2, client_count)\n",
        "    unverified_clients = random.sample(range(client_count), num_unverified)\n",
        "\n",
        "    # Type B: Insider Poisoners (Pass ZKP, but send bad updates)\n",
        "    remaining_clients = [c for c in range(client_count) if c not in unverified_clients]\n",
        "    num_poisoners = min(3, len(remaining_clients))\n",
        "    poisoning_clients = random.sample(remaining_clients, num_poisoners)\n",
        "\n",
        "    # Reset Metrics\n",
        "    excluded_clients = 0\n",
        "    local_accuracies = []\n",
        "    global_accuracies = []\n",
        "    client_auth_times = []\n",
        "    client_train_times = []\n",
        "    round_latencies = []\n",
        "    aggregation_times = []\n",
        "    communication_overheads = []\n",
        "\n",
        "    # Initialize Protocols\n",
        "    client_protocols = [SchnorrProtocol(p, g) for _ in range(client_count)]\n",
        "\n",
        "    # --- Federated Rounds ---\n",
        "    for round_num in range(rounds):\n",
        "        print(f\"  Round {round_num + 1}/{rounds}\")\n",
        "        round_start_time = time.time()\n",
        "\n",
        "        local_models = []          # For saving model size later\n",
        "        local_predictions = []     # For aggregation\n",
        "        round_client_auth_times = []\n",
        "        round_client_train_times = []\n",
        "        round_comm_overhead = 0\n",
        "\n",
        "        # --- Client Phase ---\n",
        "        for client_num, (X_c, y_c) in enumerate(client_data):\n",
        "\n",
        "            # A. Authentication (ZKP)\n",
        "            auth_start_time = time.time()\n",
        "            schnorr = client_protocols[client_num]\n",
        "            R = schnorr.generate_commitment()\n",
        "            challenge = np.random.randint(1, p)\n",
        "\n",
        "            # Simulating ZKP Failure for Unverified Clients\n",
        "            if client_num in unverified_clients:\n",
        "                response = schnorr.compute_response(challenge) + 1 # Invalid\n",
        "            else:\n",
        "                response = schnorr.compute_response(challenge) # Valid\n",
        "\n",
        "            # Verify\n",
        "            if not schnorr.verify(R, response, schnorr.public_key, challenge):\n",
        "                # print(f\"    Client {client_num + 1} failed authentication.\")\n",
        "                excluded_clients += 1\n",
        "                auth_end_time = time.time()\n",
        "                round_client_auth_times.append(auth_end_time - auth_start_time)\n",
        "                continue # Skip training for unverified clients\n",
        "\n",
        "            auth_end_time = time.time()\n",
        "            round_client_auth_times.append(auth_end_time - auth_start_time)\n",
        "\n",
        "            # B. Local Training\n",
        "            train_start_time = time.time()\n",
        "\n",
        "            # Simulating Data Poisoning (Insiders)\n",
        "            if client_num in poisoning_clients:\n",
        "                # Add noise to features\n",
        "                X_train_curr = X_c + np.random.normal(0, 2.0, X_c.shape)\n",
        "            else:\n",
        "                X_train_curr = X_c\n",
        "\n",
        "            train_data = lgb.Dataset(X_train_curr, label=y_c)\n",
        "            local_model = lgb.train(lgb_params, train_data, num_boost_round=20)\n",
        "\n",
        "            train_end_time = time.time()\n",
        "            round_client_train_times.append(train_end_time - train_start_time)\n",
        "\n",
        "            # Measure Model Size for Communication Overhead\n",
        "            local_model_bytes = pickle.dumps(local_model)\n",
        "            round_comm_overhead += len(local_model_bytes)\n",
        "            local_models.append(local_model)\n",
        "\n",
        "            # Predictions\n",
        "            preds = local_model.predict(X_test)\n",
        "\n",
        "            # Simulating Model Poisoning (Insiders send garbage predictions)\n",
        "            if client_num in poisoning_clients:\n",
        "                preds = np.random.rand(*preds.shape) # Random noise predictions\n",
        "\n",
        "            # C. Differential Privacy\n",
        "            noisy_preds = apply_laplace_noise(preds, epsilon=privacy_budget)\n",
        "            local_predictions.append(noisy_preds)\n",
        "\n",
        "            # Local Accuracy Tracking\n",
        "            local_pred_labels = np.argmax(noisy_preds, axis=1)\n",
        "            local_acc = accuracy_score(y_test, local_pred_labels)\n",
        "            local_accuracies.append(local_acc)\n",
        "\n",
        "        # --- Server Phase ---\n",
        "        if local_predictions:\n",
        "            # D. Robust Aggregation\n",
        "            agg_start_time = time.time()\n",
        "            aggregated_preds = robust_aggregation(local_predictions)\n",
        "\n",
        "            # Global Evaluation\n",
        "            y_pred_global = np.argmax(aggregated_preds, axis=1)\n",
        "            global_acc = accuracy_score(y_test, y_pred_global)\n",
        "            global_accuracies.append(global_acc)\n",
        "\n",
        "            agg_end_time = time.time()\n",
        "            aggregation_times.append(agg_end_time - agg_start_time)\n",
        "\n",
        "            # Measure Aggregated Model Size (Communication back to clients)\n",
        "            agg_model_bytes = pickle.dumps(aggregated_preds)\n",
        "            round_comm_overhead += len(agg_model_bytes)\n",
        "\n",
        "            print(f\"  Round {round_num + 1} Global Accuracy: {global_acc:.4f}\")\n",
        "        else:\n",
        "            print(\"  No valid updates received.\")\n",
        "\n",
        "        # Round Metrics\n",
        "        communication_overheads.append(round_comm_overhead)\n",
        "        round_latencies.append(time.time() - round_start_time)\n",
        "        client_auth_times.extend(round_client_auth_times)\n",
        "        client_train_times.extend(round_client_train_times)\n",
        "\n",
        "    # Save Global Model to calculate exact file size (MB)\n",
        "    if local_models:\n",
        "        model_filename = f\"global_model_{client_count}.pkl\"\n",
        "        with open(model_filename, \"wb\") as f:\n",
        "            pickle.dump(local_models[0], f)\n",
        "        global_model_size = os.path.getsize(model_filename) / (1024 ** 2) # MB\n",
        "    else:\n",
        "        global_model_size = 0\n",
        "\n",
        "    # Store Final Results\n",
        "    results.append({\n",
        "        'clients': client_count,\n",
        "        'avg_global_accuracy': np.mean(global_accuracies) if global_accuracies else 0,\n",
        "        'avg_local_accuracy': np.mean(local_accuracies) if local_accuracies else 0,\n",
        "        'avg_train_time': np.mean(client_train_times) if client_train_times else 0,\n",
        "        'avg_auth_time': np.mean(client_auth_times) if client_auth_times else 0,\n",
        "        'avg_round_latency': np.mean(round_latencies) if round_latencies else 0,\n",
        "        'avg_aggregation_time': np.mean(aggregation_times) if aggregation_times else 0,\n",
        "        'avg_comm_overhead': np.mean(communication_overheads) / (1024 ** 2) if communication_overheads else 0,\n",
        "        'global_model_size': global_model_size,\n",
        "    })"
      ],
      "metadata": {
        "id": "xckz-ZBwLymx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Final Output Printing**"
      ],
      "metadata": {
        "id": "o1SXCg5mL1dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print all results\n",
        "print(\"\\nExperiment Results:\")\n",
        "for result in results:\n",
        "    print(f\"Clients: {result['clients']}, \"\n",
        "          f\"Avg Global Accuracy: {result['avg_global_accuracy']:.5f}, \"\n",
        "          f\"Avg Local Accuracy: {result['avg_local_accuracy']:.5f}, \"\n",
        "          f\"Avg Train Time: {result['avg_train_time']:.5f}s, \"\n",
        "          f\"Avg Auth Time: {result['avg_auth_time']:.5f}s, \"\n",
        "          f\"Avg Round Latency: {result['avg_round_latency']:.5f}s, \"\n",
        "          f\"Avg Aggregation Time: {result['avg_aggregation_time']:.5f}s, \"\n",
        "          f\"Avg Comm Overhead: {result['avg_comm_overhead']:.5f}MB, \"\n",
        "          f\"Global Model Size: {result['global_model_size']:.5f}MB\")"
      ],
      "metadata": {
        "id": "sA-JrhyEL4gg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}